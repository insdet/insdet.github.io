<!DOCTYPE html>
<html>
  
<head>
  <meta charset="utf-8">
  <meta name="description" content="">
  <meta property="og:title" content="Object Instance Detection" />
  <meta property="og:description" content="" />
  <meta property="og:url" content="" />
  <meta name="keywords" content="InsDet">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <title>InsDet </title>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>


<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Object Instance Detection</h1>
            <h4 class="is-size-5 publication-authors">
              in conjunction with the 5th Workshop on <a href="https://vplow.github.io/vplow_5th.html" target="_blank">VPLOW</a>, CVPR 2025 in Nashville, USA.
            </h4>
            <br>
            <h1 class="subtitle has-text-centered">
              <a href="#overview" class="button"><b>Overview</b></a>
              <a href="#dates" class="button"><b>Important Dates</b></a>
              <a href="#instruction" class="button"><b>Instruction</b></a>
              <a href="#organizers" class="button"><b>Organizers</b></a>
              <div class="dropdown">
                <span class="button"><b>Challenges</b>&nbsp;<i class="fa fa-chevron-down" aria-hidden="true"></i></span>
                <div class="dropdown-content">
                  <a href="https://eval.ai/web/challenges/challenge-page/2478/overview">CVPR 2025</a><br />
                  <a href="https://eval.ai/web/challenges/challenge-page/2358/overview">ACCV 2024</a><br />
                  <a href="https://eval.ai/web/challenges/challenge-page/2277/overview">CVPR 2023</a>
                </div>
              </div>
            </h1>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="section" style="margin-top: -50px">
    <div class="container is-max-desktop">
      <section class="section" id="overview">
        <div class="container is-max-desktop content">
          <h2 class="title" id="overview">Overview</h2>
          <br>
          <div class="center">
            <img alt="fig1" src="https://raw.githubusercontent.com/insdet/insdet.github.io/main/static/pics/site/objdet-insdet.png">
          </div>
          <div class="content has-text-justified">
            Instance Detection (InsDet) is a practically important task in robotics applications, e.g., elderly-assistant robots need to fetch specific items from a cluttered kitchen, micro-fulfillment robots for the retail need to pick items from mixed boxes or shelves. Different from Object Detection (ObjDet) detecting all objects belonging to some predefined classes, InsDet aims to detect specific object instances defined by some examples capturing the instance from multiple views.
            <br>
            <br>
            This year, we plan to run a competition on our InsDet dataset, which is the instance detection benchmark dataset which is larger in scale and more challenging than existing InsDet datasets. The major strengths of our InsDet dataset over prior InsDet datasets include (1) both high-resolution profile images of object instances and high-resolution testing images from more realistic indoor scenes, simulating real-world indoor robots locating and recognizing object instances from a cluttered indoor scene in a distance (2) a realistic unified InsDet protocol to foster the InsDet research. 
            <br>
            <br>
            Participants in this challenge will be tasked with predicting the bounding boxes for each given instance from testing images. This exciting opportunity allows researchers, students, and data scientists to apply their expertise in computer vision and machine learning to address instance detection problem. We refer participants to the user guide for details.
          </div>            
        </div>
      </section>
    </div>
  </section>


  <section class="section" style="margin-top: -50px">
    <div class="container is-max-desktop">
      <section class="section" id="dates">
        <div class="container is-max-desktop content">
          <h2 class="title" id="dates">Important Dates</h2>
          <li><s>March 24, 2025, InsDet (6.60 GB) will be released.</s></li>
          <li><s>March 24, 2025, EvalAI server will be open.</s></li>
          <li><s>May 1, 2025, Scenes Test (2.43 GB) will be released.</s></li>
          <li>May 5, 2025, Evaluation scripts will be uploaded to github repo.</li>
          <li>June 2, 2025, Challenge will be closed.</li>
          <li>June 3, 2025, Invitation will be sent to some participants for presenting at the workshop.</li>
          <li>June 11/12, 2025, Workshop day.</li>          
        </div>
      </section>
    </div>
  </section>


    <section class="section" style="margin-top: -50px">
    <div class="container is-max-desktop">
      <section class="section" id="dataset">
        <div class="container is-max-desktop content">
          <h2 class="title" id="instruction">Instruction</h2>
          <h3>Dataset</h3>
          The dataset contains 100 object instances with multi-view profile images, 200 pure background images and 160 scene images.
          <br>
          Participants can download the dataset from the <a href="https://drive.google.com/drive/folders/1rIRTtqKJGCTifcqJFSVvFshRb-sB0OzP?usp=sharing">InsDet</a> dataset.
          <ol>
            <li><b>Objects.</b> 100 different Object instances. Each profile image has a resolution of 3072&times;3072 pixels (some instances are 3456&times;3456). Each instance is captured at 24 rotation positions (every 15&deg; in azimuth) with a 45&deg; elevation view.</li>
            <li><b>Background.</b> 200 high-resolution background images of indoor scenes that do not include any given instances from Objects.</li>
            <li><b>Scenes.</b> 160 high-resolution images (6144&times;8192) in cluttered scenes, where some instances are placed in reasonable locations. We tag these images as <i>easy</i> or <i>hard</i> based on scene clutter and object occlusion levels.</li>
          </ol>
          <br>
          <br>
          <h3>Benchmarking Protocol</h3>
          <ol>
            <li><b>Goal.</b> Developing instance detectors using profile images and optionally some random background images. The detector should detect object instances of interest in real-world testing images.</li>
            <li>
              <b>Environment for model development.</b>
              <ol type="a">
                <li>A set of object instances, each of which has some visual examples captured from multiple views. Participants should develop a model to successfully detect these object instances.</li>
                <li>Some random background images (not used in testing). Participants might use them to synthesize images. Participants can also download and use other external background images in training.</li>   
              </ol>
            </li>
            <li><b>Environment for testing.</b> Real-world indoor scene images, in which participants' algorithms should detect object instances of interest.</li>
          </ol>
          <br>
          <b>Importantly, participants are not allowed to develop any instance detectors on the Real-world indoor scene images we provided.</b> Furthermore, for participants who are invited for a presentation, we might ask for your code and models for verification. 
          <br>
          <br>
          <h3>Evaluation</h3>
          Following the COCO dataset [8], we tag testing object instances as small, medium, and large according to their bounding box area. The following 12 metrics are used for characterizing the performance of an instance detector on InsDet dataset. Additionally, we will also evaluate AP on easy and hard scenes separately.
          <br>
          <div class="center">
            <img alt="fig1" src="https://raw.githubusercontent.com/insdet/insdet.github.io/main/static/pics/site/evaluation_metrics.png">
          </div>          
        </div>
      </section>
    </div>
  </section>
  
  
 <section class="section" id="Organizers">
    <div class="container is-max-desktop content">
      <h2 class="title" id="organizers">Organizers</h2>
      <div class="columns is-centered is-variable is-0">
        <div style="display: flex">
          <div style="width:25%; justify-content: center">
            <a href="https://shenqq377.github.io/">
              <img alt="Qianqian Shen" src="static/pics/people/qianqian-shen.png" height="200"  width ="200" style =  "border-radius: 50%; object-fit: cover; ">
            </a><br>
            <a href="https://shenqq377.github.io/">Qianqian Shen</a><br>
            Zhejiang University
          </div>
        
          <div style="width:7.5%">
          </div>
       
          <div style="width:25%; justify-content: center">
            <a href="https://ics.uci.edu/~yunhaz5/">
              <img alt="Yunhan Zhao" src="static/pics/people/yunhan-zhao.png"   height="200"  width ="200" style =  "border-radius: 50%; object-fit: cover; ">
            </a><br>
            <a href="https://ics.uci.edu/~yunhaz5/">Yunhan Zhao</a><br>
            University of California, Irvine
          </div>

          <div style="width:7.5%">
          </div>
       
          <div style="width:25%; justify-content: center">
            <a href="https://aimerykong.github.io/">
              <img alt="Yunhan Zhao" src="static/pics/people/shu.jpg"   height="200"  width ="200" style =  "border-radius: 50%; object-fit: cover; ">
            </a><br>
            <a href="https://aimerykong.github.io/">Shu Kong</a><br>
            University of Macau
          </div>
          
          <div style="width:7.5%">
          </div>
        </div>
      </div>
    </div>
  </section>

  <footer class="footer">
    <div class="container">
      <div class="content">
        <p>
          It borrows the source code of <a href="https://github.com/nerfies/nerfies.github.io">this website</a>.
          We would like to thank Utkarsh Sinha and Keunhong Park.
        </p>
      </div>
    </div>
  </footer>

</body>
<script src="js/jquery-2.1.1.js"></script>
<script src="js/jquery.mobile.custom.min.js"></script> <!-- Resource jQuery -->
<script src="js/main.js"></script> <!-- Resource jQuery -->

</html>
