<!DOCTYPE html>
<html>
  
<head>
  <meta charset="utf-8">
  <meta name="description" content="">
  <meta property="og:title" content="Object Instance Detection" />
  <meta property="og:description" content="" />
  <meta property="og:url" content="" />
  <meta name="keywords" content="InsDet">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <title>InsDet </title>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>


<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Object Instance Detection</h1>
            <h4 class="is-size-5 publication-authors">
              in conjunction with the 5th Workshop on <a href="https://vplow.github.io/vplow_5th.html" target="_blank">VPLOW</a>, CVPR 2025 in Nashville, USA.
            </h4>
            
            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Dataset Link -->
                <span class="link-block">
                  <a href="https://drive.google.com/drive/folders/1rIRTtqKJGCTifcqJFSVvFshRb-sB0OzP" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fa-solid fa-database"></i>
                    </span>
                    <span>Dataset</span>
                  </a>
                </span>
                
                <!-- Challenge link -->
                <span class="link-block">
                  <a href="https://eval.ai/web/challenges/challenge-page/2478/overview" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-trophy"></i>
<!--                       <i class="fas fa-trophy-star"></i> -->
<!--                       <i class="fas fa-file-pdf"></i> -->
                    </span>
                    <span>Challenge</span>
                  </a>
                </span>

                <!-- Code link -->
                <span class="link-block">
                  <a href="https://github.com/shenqq377/instance_detection_challenge" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                
              </div> 
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Challenge -->
    <section class="section hero is-light2">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-one-half">
            <h2 class="title is-3">Overview</h2>
            <img src="static/pics/site/objdet-insdet.png" alt="1" style="width: auto; height: auto; display: block; margin: 5px auto;"/> 
            <div class="content has-text-justified">
              <p style="text-align: justify;">
                Instance Detection (InsDet) is a practically important task in robotics applications, e.g., elderly-assistant robots need to fetch specific items from a cluttered kitchen, micro-fulfillment robots for the retail need to pick items from mixed boxes or shelves. Different from Object Detection (ObjDet) detecting all objects belonging to some predefined classes, InsDet aims to detect specific object instances defined by some examples capturing the instance from multiple views.
              </p>  
              <p style="text-align: justify;">              
                This year, we plan to run a competition on our InsDet dataset, which is the instance detection benchmark dataset which is larger in scale and more challenging than existing InsDet datasets. The major strengths of our InsDet dataset over prior InsDet datasets include (1) both high-resolution profile images of object instances and high-resolution testing images from more realistic indoor scenes, simulating real-world indoor robots locating and recognizing object instances from a cluttered indoor scene in a distance (2) a realistic unified InsDet protocol to foster the InsDet research. 
              </p>  
              <p style="text-align: justify;">              
                Participants in this challenge will be tasked with predicting the bounding boxes for each given instance from testing images. This exciting opportunity allows researchers, students, and data scientists to apply their expertise in computer vision and machine learning to address instance detection problem. We refer participants to the <a href="https://docs.google.com/document/d/15R-R0tpKBy_KCNyc_8D45PQzHzyLmlPjDbntZJEBEYY/edit?tab=t.0">user guide</a> for details.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!-- End challenge -->


  <!-- Dates -->
    <section class="section hero is-light">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h2 class="title is-3">Important Dates</h2>
            <div class="content has-text-justified">
              <p style="text-align: left;">
                <li><s>March 24, 2025, InsDet (6.60 GB) will be released.</s></li>
                <li><s>March 24, 2025, EvalAI server will be open.</s></li>
                <li><s>May 1, 2025, Scenes Test (2.43 GB) will be released.</s></li>
                <li><s>May 5, 2025, Evaluation scripts will be uploaded to github repo.</s></li>
                <li>June 5, 2025, Challenge will be closed.</li>
                <li>June 6, 2025, Invitation will be sent to winners.</li>
                <li>June 11, 2025, Workshop day.</li>   
              </p>      
            </div>
          </div>
        </div>
      </div>
    </section>
  <!-- End dates -->


  <!-- Dataset -->
    <section class="section hero is-light2">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h2 class="title is-3">Dataset</h2>
            <div class="content has-text-justified">
              <p style="text-align: justify;">
                The <b>InsDet</b> dataset contains 100 object instances with multi-view profile images, 200 pure background images and 160 scene images. Participants can download the dataset from the <a href="https://drive.google.com/drive/folders/1rIRTtqKJGCTifcqJFSVvFshRb-sB0OzP?usp=sharing">InsDet</a> dataset.
              <ul>
                <li><h4><b>Objects.</b></h4>
                100 different Object instances. Each profile image has a resolution of 3072&times;3072 pixels (some instances are 3456&times;3456). Each instance is captured at 24 rotation positions (every 15&deg; in azimuth) with a 45&deg; elevation view. When capturing profile images for each instance, inspired by prior arts, we paste a QR code on the tabletop, which enables pose estimation, e.g., using COLMAP. We use the GrabCut toolbox to derive foreground masks of instances in profile images. This removes background pixels (such as QR code regions) in the profile images.
                <br>
                In practice, we center-crop foreground instances from profile images and downsize the center-crops to 1024&times;1024, and save images and masks.
                <img src="static/pics/site/vis-objects.png" alt="1" style="width: auto; height: auto; display: block; margin: 10px auto;"/>
                </li>
                <li><h4><b>Background.</b></h4>
                200 high-resolution background images of indoor scenes that do not include any given instances from Objects.</li>
                <img src="static/pics/site/vis-background.png" alt="1" style="width: auto; height: auto; display: block; margin: 10px auto;"/>
                <li><h4><b>Scenes.</b></h4> 
                160 high-resolution images (6144&times;8192) in cluttered scenes, where some instances are placed in reasonable locations. We tag these images as <i>easy</i> or <i>hard</i> based on scene clutter and object occlusion levels.</li> 
                <img src="static/pics/site/vis-scenes.png" alt="1" style="width: auto; height: auto; display: block; margin: 10px auto;"/>
              </ul>
              The <b>Scenes-Test</b> has the same structure of Scenes, but most images are captured from different scenarios. This part includes 320 high-resolution images (6144&times;8192) in cluttered scenes.
              </p>    
            </div>
          </div>
        </div>
      </div>
    </section>
  <!-- End dataset -->


  <!-- Protocol -->
    <section class="section hero is-light">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h2 class="title is-3">Benchmarking Protocol</h2>
            <div class="content has-text-justified">
              <p style="text-align: justify;">
                <ul>
                <li><h4><b>Goal.</b></h4>
                  Developing instance detectors using profile images (cf. visuals below) and optionally some random background images (e.g., to apply cut-paste-learn<sup>[1]</sup>). The detector should detect object instances of interest in real-world testing images.</li>
                <li><h4><b>Environment for model development.</b></h4>
                  <ol type="a">
                    <li>A set of object instances, each of which has some visual examples captured from multiple views. Participants should develop a model to successfully detect these object instances.</li>
                    <li>Some random background images (not used in testing). Participants might use them to synthesize images. Participants can also download and use other external background images in training.</li>
                  </ol>
                </li>
                <li><h4><b>Environment for testing.</b></h4>
                  Real-world indoor scene images, in which participants' algorithms should detect object instances of interest.</li>           
                </ul>
                <b>Importantly, participants are not allowed to develop any instance detectors on the Real-world indoor scene images we provided. <span style="color:red;">Furthermore, for participants who are invited for a 15-min presentation (online or video).</span></b>
                 
              </p>    
            </div>
          </div>
        </div>
      </div>
    </section>
  <!-- End protocol -->
  

  <!-- Evaluation -->
    <section class="section hero is-light2">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h2 class="title is-3">Evaluation & Submission</h2>
            <div class="content has-text-justified">
              <p style="text-align: justify;">
                Following the COCO dataset<sup>[8]</sup>, we tag testing object instances as small, medium, and large according to their bounding box area. The following 12 metrics are used for characterizing the performance of an instance detector on <b>InsDet</b> dataset. Additionally, we will also evaluate AP on <b>easy</b> and <b>hard</b> scenes separately. 
              </p>
              <img src="static/pics/site/evaluation_metrics.png" alt="1" style="width: 75%; height: 75%; display: block; margin: 10px auto;"/>
              <h4><b>Official Baseline</b></h4>
              <ol>
                <li><b>Faster RCNN<sup>[2]</sup> + Cut-Paste-Learn strategy<sup>[1]</sup>.</b>
                  We train Faster RCNN with data generated using the Cut-Paste-Learn strategy. The detailed hyperparameters used in data generation and model training are mentioned in the paper [3].
                </li>
                <li><b>SAM<sup>[4]</sup> + DINOv2<sup>[5]</sup> + StableMatching<sup>[6,7]</sup>.</b>
                  We proposed a simple non-learned method using the off-the-shelf class-agnostic segmentation model (Segment Anything Model, SAM), the self-supervised feature representation DINOv2 and classical StableMatching algorithm in the paper [3].
                </li>
              </ol>
              <h4><b>Submission</b></h4>
              <p style="text-align: justify;">
                The generated JSON or CSV file should adhere to the following dictionary format:
                <pre><code>[{"image_id": 0, 
  "category_id": 79, 
  "bbox": [976, 632, 64, 80], 
  "score": 99.32915569311469, 
  "image_width": 8192, 
  "image_height": 6144,
  "scale": 1, 
  "image_name": 
  "easy.leisure_zone.rgb_000.jpg"},
  ...
 {"image_id": 159, 
  "category_id": 9, 
  "bbox": [921, 803, 28, 106], 
  "score": 99.32927090665571, 
  "image_width": 8192, 
  "image_height": 6144, 
  "scale": 1, 
  "image_name": "hard.pantry_room_001.rgb_019.jpg"}]</code></pre>
              </p>
              <h4><b>References</b></h4>
              <p style="text-align: justify; font-size: 10px; line-height: 1.0; margin-top: 0px; color: #ccc;">
                <ul>[1] Dwibedi, Debidatta, et al. "Cut, paste and learn: Surprisingly easy synthesis for instance detection." Proceedings of the IEEE international conference on computer vision. 2017.</ul>
                <ul>[2] Girshick, Ross. "Fast r-cnn." Proceedings of the IEEE international conference on computer vision. 2015.</ul>
                <ul>[3] Shen, Qianqian, et al. “A High-Resolution Dataset for Instance Detection with Multi-View Instance Capture.” Thirty-seventh conference on neural information processing systems datasets and benchmarks track. 2023.</ul>
                <ul>[4] Kirillov, Alexander, et al. "Segment anything." Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.</ul>
                <ul>[5] Oquab, Maxime, et al. "Dinov2: Learning robust visual features without supervision." arXiv preprint arXiv:2304.07193 (2023).</ul>
                <ul>[6] David Gale and Lloyd S Shapley. College admissions and the stability of marriage. The American Mathematical Monthly, 69(1):9–15, 1962.</ul>
                <ul>[7] David G McVitie and Leslie B Wilson. The stable marriage problem. Communications of the ACM, 14(7):486–490, 1971.</ul>
                <ul>[8] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014.</ul>
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>
  <!-- End evaluation -->


  <!-- BibTex citation -->
  <section class="section hero is-light" id="BibTeX">
    <div class="container is-max-desktop content ">
      <div class="column is-one-half">
        <div class="column has-text-left">
          <h2 class="title">BibTeX</h2>
          <p> If you find our work useful, please consider citing our papers:</p>
          <pre><code>@inproceedings{shen2025solving,
            title={Solving Instance Detection from an Open-World Perspective},
            author={Shen, Qianqian and Zhao, Yunhan and Kwon, Nahyun and Kim, Jeeeun and Li, Yanan and Kong, Shu},
            booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
            year={2025}
            }</code></pre>
          <pre><code>@inproceedings{shen2023high,
            title={A high-resolution dataset for instance detection with multi-view object capture},
            author={Shen, Qianqian and Zhao, Yunhan and Kwon, Nahyun and Kim, Jeeeun and Li, Yanan and Kong, Shu},
            booktitle={Conference on Neural Information Processing Systems (NeurIPS) Datasets & Benchmark Track},
            year={2023}
          }</code></pre>
        </div>
      </div> 
    </div>
  </section>
  <!-- End BibTex citation -->
  
  
  <!-- Organizers -->
  <section class="section hero is-light2" id="BibTeX">
    <div class="container is-max-desktop content ">
      <div class="column is-one-half">
        <div class="column has-text-left">
          <h2 class="title">Organizers</h2>
            <div style="display: flex">
              <div style="width:25%; justify-content: center">
                <a href="https://shenqq377.github.io/">
                  <img alt="Qianqian Shen" src="static/pics/people/qianqian-shen.png" height="150"  width ="150" style =  "border-radius: 50%; object-fit: cover; ">
                </a><br>
                <a href="https://shenqq377.github.io/">Qianqian Shen</a><br>
                Zhejiang University
              </div>
        
              <div style="width:12.5%">
              </div>
       
              <div style="width:25%; justify-content: center">
                <a href="https://ics.uci.edu/~yunhaz5/">
                  <img alt="Yunhan Zhao" src="static/pics/people/yunhan-zhao.png"   height="150"  width ="150" style =  "border-radius: 50%; object-fit: cover; ">
                </a><br>
                <a href="https://ics.uci.edu/~yunhaz5/">Yunhan Zhao</a><br>
                University of California, Irvine
              </div>

              <div style="width:12.5%">
              </div>
       
              <div style="width:25%; justify-content: center">
                <a href="https://aimerykong.github.io/">
                  <img alt="Yunhan Zhao" src="static/pics/people/shu.jpg"   height="150"  width ="150" style =  "border-radius: 50%; object-fit: cover; ">
                </a><br>
                <a href="https://aimerykong.github.io/">Shu Kong</a><br>
                University of Macau
              </div>
  
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End organizers -->

</body>

</html>
